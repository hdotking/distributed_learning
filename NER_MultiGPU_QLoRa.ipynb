{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d31f401",
   "metadata": {},
   "source": [
    "# Deepspeed QLora z3 Usecase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8df73",
   "metadata": {},
   "source": [
    "The key parameters of the deepspeed config file have been described below. Highlighting what the is happening throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7ac2c",
   "metadata": {},
   "source": [
    "\n",
    "`zero_stage: 3`\n",
    "\n",
    "This enables ZeRO Stage 3 optimization, which partitions the optimizer state, gradients, and model parameters across GPUs to minimize memory consumption. This is crucial for training very large models by splitting memory-intensive components across multiple GPUs. This is related to model parallelism, as it distributes model parameters rather than duplicating them on each device`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1780aac",
   "metadata": {},
   "source": [
    "`offload_optimizer_device: none` & `offload_param_device: none`\n",
    "\n",
    "These options specify *where* the optimiser states and parameters are stored. In this case, no offloading to CPU or NVMe is done, meaning *the training will fully utilize the GPU memory*. \n",
    "Offloading is used to free up GPU memory, but here it's bypassed to keep things simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130965d",
   "metadata": {},
   "source": [
    "`zero3_save_16bit_model: true`\n",
    "\n",
    "This saves the model in 16-bit precision, helping reduce memory usage, which is key for mixed precision training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38817ea9",
   "metadata": {},
   "source": [
    "`mixed_precision: bf16`\n",
    "\n",
    "Indicates that training will use bfloat16 (bf16) precision for mixed precision training. This *reduces the required memory and computation load while maintaining enough numerical accuracy* for training deep models. This improves memory efficiency, especially useful in multi-GPU setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580379d3",
   "metadata": {},
   "source": [
    "`num_processes: 2`\n",
    "\n",
    "Specifies that* 2 processes will be used, typically one for each GPU*. In this context, this is part of data parallelism, where the model is duplicated across GPUs, and each process handles a portion of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a055b",
   "metadata": {},
   "source": [
    "`zero3_init_flag: true`\n",
    "\n",
    "This is an internal flag that ensures proper initialization of ZeRO Stage 3. It's *required* when fully leveraging the capabilities of ZeRO for memory optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac8cbf",
   "metadata": {},
   "source": [
    "`distributed_type: DEEPSPEED`\n",
    "\n",
    "This sets the distributed backend to DeepSpeed, which handles both *data and model parallelism*, optimizing memory usage and computation across GPUs. It works closely with ZeRO stages to distribute workloads efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28841507",
   "metadata": {},
   "source": [
    "`num_machines: 1`\n",
    "\n",
    "Specifies that only *one machine is used, but with multiple GPUs*. This is a typical setup for local distributed training where parallelism occurs across multiple GPUs on the same machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a43035",
   "metadata": {},
   "source": [
    "## How This Relates to Data and Model Parallelism\n",
    "\n",
    "### Data Parallelism\n",
    "By using `num_processes: 2`, each GPU handles a portion of the data, but the model is duplicated across GPUs. This is achieved through DeepSpeed Distributed Data Parallel (DDP), which synchronizes gradients across GPUs during backpropagationâ€‹. [DEEPSPEED zero3](https://deepspeed.readthedocs.io/en/latest/zero3.html)\n",
    "\n",
    "### Model Parallelism\n",
    "ZeRO Stage 3 comes into play here. Instead of duplicating the model across GPUs (which consumes a lot of memory), the model parameters and optimizer states are partitioned across GPUs. This allows you to train much larger models without running out of GPU memory.â€‹ [DEEPSPEED JSON CONFIG](https://www.deepspeed.ai/docs/config-json/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40e2b4",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1111e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1023 12:42:25.554000 96779 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[2024-10-23 12:42:25,607] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to mps (auto detect)\n",
      "W1023 12:42:26.319000 96779 torch/distributed/run.py:793] \n",
      "W1023 12:42:26.319000 96779 torch/distributed/run.py:793] *****************************************\n",
      "W1023 12:42:26.319000 96779 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1023 12:42:26.319000 96779 torch/distributed/run.py:793] *****************************************\n",
      "/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[2024-10-23 12:42:27,993] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to mps (auto detect)\n",
      "[2024-10-23 12:42:27,993] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to mps (auto detect)\n",
      "W1023 12:42:28.537000 96783 .venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "W1023 12:42:28.538000 96784 .venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[2024-10-23 12:42:28,557] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-23 12:42:28,557] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend gloo\n",
      "[2024-10-23 12:42:28,557] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/train.py\", line 158, in <module>\n",
      "[rank0]:     main(model_args, data_args, training_args)\n",
      "[rank0]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/train.py\", line 98, in main\n",
      "[rank0]:     model, peft_config, tokenizer = create_and_prepare_model(\n",
      "[rank0]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/utils.py\", line 152, in create_and_prepare_model\n",
      "[rank0]:     quant_config = QuantizationConfigBuilder(args).build()\n",
      "[rank0]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/utils.py\", line 79, in build\n",
      "[rank0]:     self._build_4bit_config()\n",
      "[rank0]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/utils.py\", line 91, in _build_4bit_config\n",
      "[rank0]:     self.bnb_config = BitsAndBytesConfig(\n",
      "[rank0]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 402, in __init__\n",
      "[rank0]:     self.post_init()\n",
      "[rank0]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 460, in post_init\n",
      "[rank0]:     if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n",
      "[rank0]:   File \"/Users/hdot/.pyenv/versions/3.10.12/lib/python3.10/importlib/metadata/__init__.py\", line 996, in version\n",
      "[rank0]:     return distribution(distribution_name).version\n",
      "[rank0]:   File \"/Users/hdot/.pyenv/versions/3.10.12/lib/python3.10/importlib/metadata/__init__.py\", line 969, in distribution\n",
      "[rank0]:     return Distribution.from_name(distribution_name)\n",
      "[rank0]:   File \"/Users/hdot/.pyenv/versions/3.10.12/lib/python3.10/importlib/metadata/__init__.py\", line 548, in from_name\n",
      "[rank0]:     raise PackageNotFoundError(name)\n",
      "[rank0]: importlib.metadata.PackageNotFoundError: No package metadata was found for bitsandbytes\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/train.py\", line 158, in <module>\n",
      "[rank1]:     main(model_args, data_args, training_args)\n",
      "[rank1]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/train.py\", line 98, in main\n",
      "[rank1]:     model, peft_config, tokenizer = create_and_prepare_model(\n",
      "[rank1]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/utils.py\", line 152, in create_and_prepare_model\n",
      "[rank1]:     quant_config = QuantizationConfigBuilder(args).build()\n",
      "[rank1]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/utils.py\", line 79, in build\n",
      "[rank1]:     self._build_4bit_config()\n",
      "[rank1]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/utils.py\", line 91, in _build_4bit_config\n",
      "[rank1]:     self.bnb_config = BitsAndBytesConfig(\n",
      "[rank1]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 402, in __init__\n",
      "[rank1]:     self.post_init()\n",
      "[rank1]:   File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 460, in post_init\n",
      "[rank1]:     if self.load_in_4bit and not version.parse(importlib.metadata.version(\"bitsandbytes\")) >= version.parse(\n",
      "[rank1]:   File \"/Users/hdot/.pyenv/versions/3.10.12/lib/python3.10/importlib/metadata/__init__.py\", line 996, in version\n",
      "[rank1]:     return distribution(distribution_name).version\n",
      "[rank1]:   File \"/Users/hdot/.pyenv/versions/3.10.12/lib/python3.10/importlib/metadata/__init__.py\", line 969, in distribution\n",
      "[rank1]:     return Distribution.from_name(distribution_name)\n",
      "[rank1]:   File \"/Users/hdot/.pyenv/versions/3.10.12/lib/python3.10/importlib/metadata/__init__.py\", line 548, in from_name\n",
      "[rank1]:     raise PackageNotFoundError(name)\n",
      "[rank1]: importlib.metadata.PackageNotFoundError: No package metadata was found for bitsandbytes\n",
      "E1023 12:42:29.048000 96779 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 96783) of binary: /Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1153, in launch_command\n",
      "    deepspeed_launcher(args)\n",
      "  File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 846, in deepspeed_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/Users/hdot/code/Multi-GPU-Fine-Training-LLMs-main/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-10-23_12:42:29\n",
      "  host      : harpals-mbp.lan\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 96784)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-10-23_12:42:29\n",
      "  host      : harpals-mbp.lan\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 96783)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file \"deepspeed_config_z3_qlora.yaml\" train.py \\\n",
    "--seed 100 \\\n",
    "--model_name_or_path \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \\\n",
    "--dataset_name \"DFKI-SLT/cross_ner\" \\\n",
    "--splits \"train,test,validation\" \\\n",
    "--max_seq_len 2000 \\\n",
    "--num_train_epochs 1 \\\n",
    "--logging_steps 5 \\\n",
    "--log_level \"info\" \\\n",
    "--logging_strategy \"steps\" \\\n",
    "--evaluation_strategy \"epoch\" \\\n",
    "--save_strategy \"epoch\" \\\n",
    "--bf16 True \\\n",
    "--learning_rate 1e-4 \\\n",
    "--lr_scheduler_type \"cosine\" \\\n",
    "--weight_decay 1e-4 \\\n",
    "--warmup_ratio 0.0 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--output_dir \"llama-sft-qlora-dsz3\" \\\n",
    "--per_device_train_batch_size 2 \\\n",
    "--per_device_eval_batch_size 2 \\\n",
    "--gradient_accumulation_steps 2 \\\n",
    "--gradient_checkpointing True \\\n",
    "--use_reentrant True \\\n",
    "--dataset_text_field \"content\" \\\n",
    "--use_flash_attn True \\\n",
    "--use_peft_lora True \\\n",
    "--lora_r 8 \\\n",
    "--lora_alpha 16 \\\n",
    "--lora_dropout 0.1 \\\n",
    "--lora_target_modules \"all-linear\" \\\n",
    "--use_4bit_quantization True \\\n",
    "--use_nested_quant True \\\n",
    "--bnb_4bit_compute_dtype \"bfloat16\" \\\n",
    "--bnb_4bit_quant_storage_dtype \"bfloat16\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741f3a3f-6a7e-4c78-a5da-50e0a6027a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q pip install torch transformers datasets peft trl accelerate deepspeed bitsandbytes flash-attn --no-build-isolation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
